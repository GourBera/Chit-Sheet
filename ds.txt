Regression => 
A measure of the relation between the mean value of one variable (e.g. output) and corresponding values of other variables (e.g. time and cost).

Hyper-parameter => 
Decision tree --> depth
SVM  --> kernel / C (gama) # Grid search cross validation


Music app - https://www1.brain.fm/app
https://lagunita.stanford.edu/courses/DB/SQL/SelfPaced/courseware/ch-sql/seq-vid-introduction_to_sql/?child=first
https://lagunita.stanford.edu/courses/DB/SQL/SelfPaced/courseware/ch-sql/seq-exercise-sql_movie_query_core/











WEKA

https://www.youtube.com/watch?v=5FUdrBq-WFo&list=PL9ooVrP1hQOFP9H8Y15DVGCA6GavhgJ8a
https://www.youtube.com/watch?v=9YGchzL-yFw

https://www.youtube.com/watch?v=ExLz-PGgGQI
C:\Users\berag\eclipse-workspace\Test1
https://www.youtube.com/user/edurekaIN/playlists
proxy.ind.hp.com 
15.85.199.199
L(N3)
https://www.youtube.com/watch?v=IW6t-qyZVo0
https://www.youtube.com/watch?v=IVshUOPqGoc
https://www.youtube.com/watch?v=jIKqxzko7Ws
https://www.youtube.com/channel/UCugcwROAgYkV8WbDy7UKcrg
https://www.youtube.com/channel/UCgx5SDcUQWCQ_1CNneQzCRw/playlists
https://www.youtube.com/watch?v=8OoSK_RWUe4
https://www.youtube.com/watch?v=ueq1iTWQU5U
https://www.youtube.com/watch?v=juRKu9cBwQ0&t=447s
https://www.youtube.com/watch?v=T1nObEOyWHs
https://www.youtube.com/watch?v=cNKi0Pl5wbc
https://www.youtube.com/watch?v=0mm3djZ5KpQ&list=PL6tu16kXT9Pp3NFZgLbPZXEykeGQwxGSx&index=2












89000+89000+1147+15000+25000

219147
033991030
8193371#
1702000056
9083253349 - SMU Project Steering Committee
Vamsikrishna.B01@mphasis.com
https://www.qasymphony.com/blog/64-test-metrics/

Dataprovider-limitation
parasoft training
open gmail in new window
@February2017
206.122.3.250
15.85.199.199
test lab
https://www.youtube.com/watch?v=Ri9jm3WDixE&list=RDH_-ikay9HoU
















gourbera@yahoo.com
PaypalPass@218

AYGPB4392N
SlackPass@218

Regularization l1, l2
unstructure data classification
api development to derive data
high biyas vs high variance =>

high biyas (underfit) -> Training error and validation error will be high
high variance (overfit) -> Training error will be low, but validation error will be high


Uniform distribution -> no mode (uniformly distributed, all data points are same)
Median is best for highly skewed distribution

TF-IDF =>
Term Frequency Inverse Document Frequency

Normalization = feature - (subtracting the mean and then dividing the std)
TF-IDF, n-gram
Fourier Transformation
ASCII
logging

NLTK, pyaudio, struct, Textblob, 



df.column
df.keys()
list(df)



bias (how well the model fits the data) and variance (how much the model changes based on changes in the inputs).



df.query('action == "click"').id.nunique()
df[['A','B','C','D']] = pd.get_dummies(df.prestige)



df.drop(['prestige','D'],axis = 1, inplace=True)



df['intercept'] = 1

logit_mod = sm.Logit(df['admit'], df[['intercept', 'gre','gpa','A','B','C']])
result = logit_mod.fit()
result.summary()

df['intercept'] = 1

mod = sm.OLS(df.price, df[['intercept','carats']])
result = mod.fit()
result.summary()



MSCS



import statsmodels.api as sm

data['intercept'] = 1

lm = sm.OLS(data['Detergents_Paper'], data[['intercept','Fresh', 'Milk', 'Grocery', 'Frozen', 'Delicatessen']])
results = lm.fit()
results.summary()



df[['A','B','C']] = pd.get_dummies(df['neighborhood'])
df[['ranch','victorian','lodge']] = pd.get_dummies(df['style'])

df['style'].unique()

df['intercept'] = 1
lm = sm.OLS( df['price'], df[['area', 'bedrooms', 'bathrooms','A','B','C','ranch','victorian','lodge']])
result = lm.fit()
summary = result.summary()
summary



def dummy_cat(df, col):
    '''
    INPUT:
    df - the dataframe where col is stored
    col - the categorical column you want to dummy (as a string)
    OUTPUT:
    df - the dataframe with the added columns
         for dummy variables using 1, 0, -1 coding
    '''
    for idx, val_0 in enumerate(df[col].unique()):
        if idx + 1 < df[col].nunique():            
            df[val_0] = df[col].apply(lambda x: 1 if x == val_0 else 0)
        else:    
            df[val_0] = df[col].apply(lambda x: -1 if x == val_0 else 0)
            for idx, val_1 in enumerate(df[col].unique()):
                if idx + 1 < df[col].nunique():
                    df[val_1] = df[val_0] + df[val_1]
                else:
                    del df[val_1]
    return df


new_df = dummy_cat(df, 'style') # Use on style
new_df.head(10)

new_df['intercept'] = 1

lm = sm.OLS(new_df['price'], new_df[['intercept', 'ranch', 'victorian']])
results = lm.fit()
results.summary()


style_dummies = pd.get_dummies(df['style'])
new_df2 = df2.join(style_dummies)
new_df2.head(10)


new_df2['intercept'] = 1

lm2 = sm.OLS(new_df2['price'], new_df2[['intercept', 'ranch', 'victorian']])
results2 = lm2.fit()
results2.summary()








log_mod = LogisticRegression()
log_mod.fit(X_train, y_train)
preds = log_mod.predict(X_test)
confusion_matrix(y_test, preds) 


precision_score(y_test, preds) 

recall_score(y_test, preds)

accuracy_score(y_test, preds)

from ggplot import *
from sklearn.metrics import roc_curve, auc
%matplotlib inline

preds = log_mod.predict_proba(X_test)[:,1]
fpr, tpr, _ = roc_curve(y_test, preds)

df = pd.DataFrame(dict(fpr=fpr, tpr=tpr))
ggplot(df, aes(x='fpr', y='tpr')) +\
    geom_line() +\
    geom_abline(linetype='dashed')













https://career-resource-center.udacity.com/interview-courses-and-guides/machine-learning
https://career-resource-center.udacity.com/interviews
https://docs.google.com/document/d/1TNLwa_ThJlliCk6Dk-_bkZU3ZMZOTEmezE2eX7i9rEg/pub?embedded=true
https://elitedatascience.com/machine-learning-interview-questions-answers
https://interviewing.io
https://www.interviewbit.com






















 
